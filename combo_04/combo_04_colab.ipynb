{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Combo 04: Encoder-Based GAN Inversion\n",
        "\n",
        "**Configuration:**\n",
        "- **Resolution:** 1024Ã—1024\n",
        "- **Latent Space:** W+\n",
        "- **Loss:** LPIPS (Perceptual)\n",
        "- **Initialization:** e4e Encoder\n",
        "- **Optimization:** Adam (300 steps, LR=0.01)\n",
        "\n",
        "This notebook demonstrates encoder-based initialization for GAN inversion using the [Encoder for Editing (e4e)](https://github.com/omertov/encoder4editing) framework.\n",
        "\n",
        "---\n",
        "\n",
        "## How It Works\n",
        "\n",
        "1. **Clone your GAN_Inversion project** (includes external/encoder4editing)\n",
        "2. **Encoder Init:** Use e4e encoder to predict initial latent code from target image\n",
        "3. **Optimization:** Refine latent code using Adam + LPIPS loss\n",
        "4. **Results:** Save to Google Drive for comparison with Combos 1-3\n",
        "\n",
        "---\n",
        "\n",
        "## Instructions\n",
        "\n",
        "1. **Run all cells** (Runtime â†’ Run all)\n",
        "2. **Wait** for setup (cloning project, installing dependencies, downloading checkpoint)\n",
        "3. **Results** will be saved to `MyDrive/GAN_Inversion_Results/combo_04/`\n",
        "\n",
        "The notebook will automatically:\n",
        "- Clone the project from `https://github.com/assafzimand/GAN_Inversion`\n",
        "- Clone e4e repository into `external/encoder4editing`\n",
        "- Load sample images from `data/samples/`\n",
        "- Download e4e checkpoint (~350 MB)\n",
        "- Save all results to your Google Drive\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup: Mount Google Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create output directory\n",
        "output_base = '/content/drive/MyDrive/GAN_Inversion_Results/combo_04'\n",
        "os.makedirs(output_base, exist_ok=True)\n",
        "\n",
        "print(f\"Google Drive mounted successfully!\")\n",
        "print(f\"Results will be saved to: {output_base}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Clone Repositories\n",
        "\n",
        "Clones your GAN_Inversion project + e4e encoder repository\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "# Clone YOUR main project\n",
        "if not Path('GAN_Inversion').exists():\n",
        "    print(\"Cloning GAN_Inversion project...\")\n",
        "    !git clone https://github.com/assafzimand/GAN_Inversion.git GAN_Inversion\n",
        "    print(\"âœ“ Main project cloned!\")\n",
        "else:\n",
        "    print(\"âœ“ Main project already exists\")\n",
        "\n",
        "# Change to project directory\n",
        "os.chdir('/content/GAN_Inversion')\n",
        "print(f\"âœ“ Working directory: {os.getcwd()}\")\n",
        "\n",
        "# Clone e4e repository into external/ (if not present)\n",
        "os.makedirs('external', exist_ok=True)\n",
        "if not Path('external/encoder4editing').exists():\n",
        "    print(\"\\nCloning e4e repository into external/...\")\n",
        "    !git clone https://github.com/omertov/encoder4editing.git external/encoder4editing\n",
        "    print(\"âœ“ e4e repository cloned!\")\n",
        "else:\n",
        "    print(\"âœ“ e4e repository already exists\")\n",
        "\n",
        "# Add e4e to path\n",
        "sys.path.insert(0, '/content/GAN_Inversion/external/encoder4editing')\n",
        "print(\"âœ“ e4e added to Python path\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Install Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install only the required dependencies (Colab already has torch/torchvision)\n",
        "print(\"Installing dependencies...\")\n",
        "%pip install -q lpips gdown Pillow numpy matplotlib PyYAML ninja\n",
        "\n",
        "# Verify torch is available\n",
        "import torch\n",
        "print(f\"\\nâœ“ Using PyTorch {torch.__version__}\")\n",
        "print(f\"âœ“ CUDA available: {torch.cuda.is_available()}\")\n",
        "print(\"âœ“ All dependencies ready!\")\n",
        "\n",
        "# Note: e4e will compile CUDA extensions on first import (may take 1-2 minutes)\n",
        "print(\"\\nNote: e4e CUDA operations will compile on first use (this is normal)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Download e4e Checkpoint\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gdown\n",
        "\n",
        "# Create checkpoints directory in combo_04/\n",
        "os.makedirs('combo_04/checkpoints', exist_ok=True)\n",
        "\n",
        "# Download e4e FFHQ encoder checkpoint\n",
        "checkpoint_path = 'combo_04/checkpoints/e4e_ffhq_encode.pt'\n",
        "\n",
        "if not os.path.exists(checkpoint_path):\n",
        "    print(\"Downloading e4e checkpoint (may take a few minutes)...\")\n",
        "    url = 'https://drive.google.com/uc?id=1cUv_reLE6k3604or78EranS7XzuVMWeO'\n",
        "    gdown.download(url, checkpoint_path, quiet=False)\n",
        "    print(\"âœ“ Checkpoint downloaded successfully!\")\n",
        "else:\n",
        "    print(\"âœ“ Checkpoint already exists\")\n",
        "\n",
        "print(f\"Checkpoint ready at: {checkpoint_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Load Target Images\n",
        "\n",
        "Loading sample FFHQ images from `data/samples/` (included in the repo).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "\n",
        "# Load images from data/samples/\n",
        "samples_dir = Path('data/samples')\n",
        "image_paths = list(samples_dir.glob('*.png')) + list(samples_dir.glob('*.jpg'))\n",
        "\n",
        "print(f\"Found {len(image_paths)} images in data/samples/\\n\")\n",
        "\n",
        "# Load and resize images\n",
        "images = []\n",
        "image_names = []\n",
        "\n",
        "for img_path in image_paths:\n",
        "    img = Image.open(img_path).convert('RGB')\n",
        "    # Resize to 1024x1024 if needed\n",
        "    if img.size != (1024, 1024):\n",
        "        img = img.resize((1024, 1024), Image.BICUBIC)\n",
        "    images.append(img)\n",
        "    image_names.append(img_path.name)\n",
        "\n",
        "print(f\"Loaded {len(images)} image(s):\")\n",
        "for name in image_names:\n",
        "    print(f\"  - {name}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.5. Clear CUDA Extension Cache\n",
        "\n",
        "This ensures e4e CUDA operations compile cleanly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "# Clear torch extensions cache\n",
        "cache_dir = Path.home() / '.cache' / 'torch_extensions'\n",
        "if cache_dir.exists():\n",
        "    print(\"Clearing torch extensions cache...\")\n",
        "    shutil.rmtree(cache_dir)\n",
        "    print(\"âœ“ Cache cleared\")\n",
        "else:\n",
        "    print(\"âœ“ No cache to clear\")\n",
        "\n",
        "print(\"\\ne4e CUDA operations will compile on first import (takes 1-2 minutes)...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Load e4e Model\n",
        "\n",
        "Using their official `setup_model()` function to load the complete pSp framework (encoder + decoder).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from utils.model_utils import setup_model\n",
        "\n",
        "# Set device\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load e4e model\n",
        "print(\"\\nLoading e4e model...\")\n",
        "model, opts = setup_model(checkpoint_path, device)\n",
        "model.eval()\n",
        "\n",
        "print(\"Model loaded successfully!\")\n",
        "print(f\"  - Encoder type: {opts.encoder_type}\")\n",
        "print(f\"  - StyleGAN size: {opts.stylegan_size}\")\n",
        "print(f\"  - Model has encoder: {hasattr(model, 'encoder')}\")\n",
        "print(f\"  - Model has decoder: {hasattr(model, 'decoder')}\")\n",
        "print(f\"  - Model has latent_avg: {hasattr(model, 'latent_avg')}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Run Inversion: Encoder Init + Optimization\n",
        "\n",
        "This is our **custom optimization loop** using:\n",
        "- **Encoder initialization** (from e4e)\n",
        "- **LPIPS loss** (perceptual)\n",
        "- **Adam optimizer** (300 steps, LR=0.01)\n",
        "- **e4e decoder** for generation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "import lpips\n",
        "import numpy as np\n",
        "from torch.optim import Adam\n",
        "from torchvision import transforms\n",
        "import time\n",
        "\n",
        "# Configuration (Combo 4: encoder-based initialization)\n",
        "CONFIG = {\n",
        "    'steps': 300,\n",
        "    'learning_rate': 0.01,\n",
        "    'betas': (0.9, 0.999),\n",
        "    'log_interval': 50,\n",
        "    'save_interval': 100,  # Save at 0, 100, 200\n",
        "}\n",
        "\n",
        "# Initialize LPIPS loss\n",
        "lpips_loss_fn = lpips.LPIPS(net='alex').to(device)\n",
        "lpips_loss_fn.eval()\n",
        "\n",
        "# Image preprocessing (PIL to tensor)\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "# Results storage\n",
        "results = []\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(f\"Starting Combo 4: Processing {len(images)} image(s)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Process all images\n",
        "for idx, (img, img_name) in enumerate(zip(images, image_names)):\n",
        "    print(f\"\\n[{idx+1}/{len(images)}] Processing: {img_name}\")\n",
        "    print(\"-\"*80)\n",
        "    \n",
        "    # Convert image to tensor [1, 3, 1024, 1024]\n",
        "    target_1024 = transform(img).unsqueeze(0).to(device)\n",
        "    \n",
        "    # Resize to 256Ã—256 (what e4e encoder expects)\n",
        "    target_256 = F.interpolate(target_1024, size=(256, 256), mode='bilinear', align_corners=False)\n",
        "    print(f\"  Resized input: {target_1024.shape} â†’ {target_256.shape}\")\n",
        "    \n",
        "    # Step 1: Encoder initialization (using e4e's full API with resize=True)\n",
        "    print(\"Step 1: Encoder initialization...\")\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        # Use their full forward API (handles encoder + latent_avg + decoder + face_pool)\n",
        "        initial_image, initial_latent = model(target_256, resize=True, randomize_noise=False, return_latents=True)\n",
        "    \n",
        "    print(f\"  Output latent shape: {initial_latent.shape}\")\n",
        "    print(f\"  Output image shape: {initial_image.shape}\")\n",
        "    print(f\"  Output image stats: min={initial_image.min():.3f}, max={initial_image.max():.3f}, mean={initial_image.mean():.3f}\")\n",
        "    \n",
        "    # Step 2: Optimization\n",
        "    print(f\"Step 2: Optimizing with Adam (LR={CONFIG['learning_rate']}, Steps={CONFIG['steps']})...\")\n",
        "    \n",
        "    # Make latent optimizable\n",
        "    latent = initial_latent.detach().clone().requires_grad_(True)\n",
        "    \n",
        "    # Create optimizer\n",
        "    optimizer = Adam([latent], lr=CONFIG['learning_rate'], betas=CONFIG['betas'])\n",
        "    \n",
        "    # Track history\n",
        "    loss_history = []\n",
        "    intermediates = {0: initial_image.cpu()}  # Store encoder output as Step 0\n",
        "    print(f\"  Stored Step 0 = encoder output (not optimization step)\")\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Optimization loop\n",
        "    for step in range(CONFIG['steps']):\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Generate image from latent (decoder outputs 1024Ã—1024, face_pool â†’ 256Ã—256)\n",
        "        generated, _ = model.decoder([latent], input_is_latent=True, randomize_noise=False, return_latents=True)\n",
        "        generated = model.face_pool(generated)  # Apply face_pool to get 256Ã—256\n",
        "        \n",
        "        # Compute LPIPS loss (comparing 256Ã—256 images)\n",
        "        loss = lpips_loss_fn(generated, target_256).mean()\n",
        "        \n",
        "        # Backward\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Track\n",
        "        loss_history.append(loss.item())\n",
        "        \n",
        "        # Save intermediates (skip step 0 since we already have encoder output)\n",
        "        if step > 0 and step % CONFIG['save_interval'] == 0:\n",
        "            with torch.no_grad():\n",
        "                intermediate, _ = model.decoder([latent], input_is_latent=True, randomize_noise=False, return_latents=True)\n",
        "                intermediate = model.face_pool(intermediate)\n",
        "                intermediates[step] = intermediate.cpu()\n",
        "        \n",
        "        # Log\n",
        "        if (step + 1) % CONFIG['log_interval'] == 0 or step == 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            print(f\"  Step [{step+1}/{CONFIG['steps']}] Loss: {loss.item():.6f} Time: {elapsed:.2f}s\")\n",
        "    \n",
        "    # Final generation\n",
        "    with torch.no_grad():\n",
        "        final_output, _ = model.decoder([latent], input_is_latent=True, randomize_noise=False, return_latents=True)\n",
        "        final_output = model.face_pool(final_output)\n",
        "        intermediates[CONFIG['steps']-1] = final_output.cpu()\n",
        "    \n",
        "    total_time = time.time() - start_time\n",
        "    \n",
        "    print(f\"\\nCompleted in {total_time:.2f}s\")\n",
        "    print(f\"Final loss: {loss_history[-1]:.6f}\")\n",
        "    print(f\"Initial loss: {loss_history[0]:.6f}\")\n",
        "    print(f\"Improvement: {loss_history[0] - loss_history[-1]:.6f}\")\n",
        "    \n",
        "    # Store results\n",
        "    results.append({\n",
        "        'name': img_name,\n",
        "        'target': target_256.cpu(),  # Store 256Ã—256 target for visualization\n",
        "        'initial_latent': initial_latent.detach().cpu(),\n",
        "        'final_latent': latent.detach().cpu(),\n",
        "        'final_output': final_output.cpu(),\n",
        "        'loss_history': loss_history,\n",
        "        'intermediates': intermediates,\n",
        "        'time': total_time\n",
        "    })\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(f\"âœ“ Combo 4 complete! Processed {len(results)} image(s)\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Visualize Results\n",
        "\n",
        "Creating evolution panels showing the progression from encoder init to final optimization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, Image as IPImage\n",
        "import io\n",
        "\n",
        "def tensor_to_image(tensor):\n",
        "    \"\"\"Convert tensor [-1, 1] to numpy image [0, 255]\"\"\"\n",
        "    # Handle any tensor shape, ensure it's [C, H, W]\n",
        "    img = tensor.cpu().squeeze()  # Remove all size-1 dimensions\n",
        "    \n",
        "    # If still has batch dimension, take first item\n",
        "    while img.dim() > 3:\n",
        "        img = img[0]\n",
        "    \n",
        "    # Should now be [C, H, W], convert to [H, W, C]\n",
        "    if img.dim() == 3:\n",
        "        img = img.permute(1, 2, 0)\n",
        "    \n",
        "    img = img.numpy()\n",
        "    img = (img * 0.5 + 0.5) * 255  # Denormalize from [-1, 1] to [0, 255]\n",
        "    img = np.clip(img, 0, 255).astype(np.uint8)\n",
        "    return img\n",
        "\n",
        "# Visualize each result\n",
        "for result in results:\n",
        "    img_name = result['name']\n",
        "    target = result['target']\n",
        "    intermediates = result['intermediates']\n",
        "    loss_history = result['loss_history']\n",
        "    \n",
        "    # Get intermediate steps (0, 100, 200, final)\n",
        "    steps_to_show = sorted(intermediates.keys())\n",
        "    \n",
        "    # Create evolution panel\n",
        "    num_images = len(steps_to_show) + 1  # +1 for original\n",
        "    fig, axes = plt.subplots(1, num_images, figsize=(4*num_images, 4))\n",
        "    \n",
        "    # Original\n",
        "    axes[0].imshow(tensor_to_image(target))\n",
        "    axes[0].set_title('Original', fontsize=12, fontweight='bold')\n",
        "    axes[0].axis('off')\n",
        "    \n",
        "    # Intermediates\n",
        "    for idx, step in enumerate(steps_to_show):\n",
        "        axes[idx+1].imshow(tensor_to_image(intermediates[step]))\n",
        "        axes[idx+1].set_title(f'Step {step}', fontsize=12)\n",
        "        axes[idx+1].axis('off')\n",
        "    \n",
        "    # Add metrics to title\n",
        "    final_loss = loss_history[-1]\n",
        "    initial_loss = loss_history[0]\n",
        "    improvement = initial_loss - final_loss\n",
        "    \n",
        "    fig.suptitle(\n",
        "        f'{img_name} | LPIPS: {final_loss:.4f} | Improvement: {improvement:.4f}',\n",
        "        fontsize=14,\n",
        "        fontweight='bold'\n",
        "    )\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    display(fig)  # Force display in Colab\n",
        "    plt.show()\n",
        "    plt.close(fig)  # Clean up\n",
        "    \n",
        "    # Plot loss curve\n",
        "    fig2 = plt.figure(figsize=(10, 4))\n",
        "    plt.plot(loss_history, linewidth=2)\n",
        "    plt.xlabel('Step', fontsize=12)\n",
        "    plt.ylabel('LPIPS Loss', fontsize=12)\n",
        "    plt.title(f'Loss Curve - {img_name}', fontsize=14, fontweight='bold')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    display(fig2)  # Force display in Colab\n",
        "    plt.show()\n",
        "    plt.close(fig2)  # Clean up\n",
        "    \n",
        "    print(f\"âœ“ Visualized: {img_name}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Save Results to Google Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "# Create timestamped output directory\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "output_dir = os.path.join(output_base, f'run_{timestamp}')\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "print(f\"Saving results to: {output_dir}\\n\")\n",
        "\n",
        "for result in results:\n",
        "    img_name = result['name'].rsplit('.', 1)[0]  # Remove extension\n",
        "    \n",
        "    # Create subdirectory for this image\n",
        "    img_dir = os.path.join(output_dir, img_name)\n",
        "    os.makedirs(img_dir, exist_ok=True)\n",
        "    \n",
        "    # Save final reconstruction\n",
        "    final_img = tensor_to_image(result['final_output'])\n",
        "    final_img_pil = Image.fromarray(final_img)\n",
        "    final_img_pil.save(os.path.join(img_dir, f'{img_name}_reconstruction.png'))\n",
        "    \n",
        "    # Save evolution panel\n",
        "    steps_to_show = sorted(result['intermediates'].keys())\n",
        "    num_images = len(steps_to_show) + 1\n",
        "    fig, axes = plt.subplots(1, num_images, figsize=(4*num_images, 4))\n",
        "    \n",
        "    axes[0].imshow(tensor_to_image(result['target']))\n",
        "    axes[0].set_title('Original', fontsize=12, fontweight='bold')\n",
        "    axes[0].axis('off')\n",
        "    \n",
        "    for idx, step in enumerate(steps_to_show):\n",
        "        axes[idx+1].imshow(tensor_to_image(result['intermediates'][step]))\n",
        "        axes[idx+1].set_title(f'Step {step}', fontsize=12)\n",
        "        axes[idx+1].axis('off')\n",
        "    \n",
        "    final_loss = result['loss_history'][-1]\n",
        "    initial_loss = result['loss_history'][0]\n",
        "    improvement = initial_loss - final_loss\n",
        "    \n",
        "    fig.suptitle(\n",
        "        f'{img_name} | LPIPS: {final_loss:.4f} | Improvement: {improvement:.4f}',\n",
        "        fontsize=14,\n",
        "        fontweight='bold'\n",
        "    )\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(img_dir, f'{img_name}_evolution.png'), dpi=150, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    \n",
        "    # Save loss curve\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.plot(result['loss_history'], linewidth=2)\n",
        "    plt.xlabel('Step', fontsize=12)\n",
        "    plt.ylabel('LPIPS Loss', fontsize=12)\n",
        "    plt.title(f'Loss Curve - {img_name}', fontsize=14, fontweight='bold')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(img_dir, f'{img_name}_loss_curve.png'), dpi=150, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    \n",
        "    # Save metrics\n",
        "    metrics = {\n",
        "        'image_name': result['name'],\n",
        "        'initial_loss': float(initial_loss),\n",
        "        'final_loss': float(final_loss),\n",
        "        'improvement': float(improvement),\n",
        "        'optimization_time': result['time'],\n",
        "        'num_steps': len(result['loss_history'])\n",
        "    }\n",
        "    \n",
        "    with open(os.path.join(img_dir, f'{img_name}_metrics.json'), 'w') as f:\n",
        "        json.dump(metrics, f, indent=2)\n",
        "    \n",
        "    # Save loss history\n",
        "    with open(os.path.join(img_dir, f'{img_name}_loss_history.json'), 'w') as f:\n",
        "        json.dump(result['loss_history'], f)\n",
        "    \n",
        "    print(f\"âœ“ Saved: {img_name}\")\n",
        "\n",
        "# Save summary\n",
        "summary = {\n",
        "    'timestamp': timestamp,\n",
        "    'num_images': len(results),\n",
        "    'config': CONFIG,\n",
        "    'results': [\n",
        "        {\n",
        "            'name': r['name'],\n",
        "            'initial_loss': float(r['loss_history'][0]),\n",
        "            'final_loss': float(r['loss_history'][-1]),\n",
        "            'time': r['time']\n",
        "        }\n",
        "        for r in results\n",
        "    ]\n",
        "}\n",
        "\n",
        "with open(os.path.join(output_dir, 'summary.json'), 'w') as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"All results saved to:\")\n",
        "print(f\"  {output_dir}\")\n",
        "print(f\"{'='*80}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## âœ… Complete!\n",
        "\n",
        "### Summary\n",
        "\n",
        "You've successfully run **Combo 04: Encoder-Based GAN Inversion** with:\n",
        "- âœ… e4e encoder for initialization\n",
        "- âœ… LPIPS perceptual loss\n",
        "- âœ… Adam optimization (300 steps)\n",
        "- âœ… 1024Ã—1024 resolution\n",
        "\n",
        "### Results Location\n",
        "\n",
        "All results have been saved to your Google Drive:\n",
        "- **Path:** `MyDrive/GAN_Inversion_Results/combo_04/run_YYYYMMDD_HHMMSS/`\n",
        "- **Contains:** Reconstructions, evolution panels, loss curves, metrics\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "1. **Download results** from your Google Drive\n",
        "2. **Compare with Combos 1-3** (128Ã—128, random/mean init)\n",
        "3. **Analyze metrics** to see the benefit of encoder initialization\n",
        "\n",
        "### Key Observations\n",
        "\n",
        "- **Encoder init** provides a better starting point than random/mean\n",
        "- **Optimization** further refines the reconstruction\n",
        "- **LPIPS loss** focuses on perceptual quality over pixel-perfect matching\n",
        "\n",
        "---\n",
        "\n",
        "**Thank you for using Combo 04!** ðŸŽ‰\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
